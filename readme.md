# ResNet 实现经验记录

## 1. ResidualBlock 的实现思路

### 1.1 基本结构

一个残差块的流程：
输入 `x → Conv → BN → ReLU → Conv → BN → 残差相加 → ReLU → 输出`

### 1.2 实现要点

* **残差形状对齐**

  * 如果 `in_channel ≠ out_channel`，用 `1×1 conv` 调整通道数。
  * 如果因为 `stride` 或 `padding` 导致空间分辨率变化（常见的是 `1/2`），可以用 `stride=2` 的 `1×1 conv` 对残差分支下采样。

* **激活函数位置**

  * 第二次 ReLU 在 **残差相加之后**，而不是卷积之后。
  * 残差分支（shortcut）本身也要经过 BN，保证数值稳定。

* **卷积配置**

  * 残差块内部通常有两个卷积：

    * 第一次 `stride=2`（做下采样）；
    * 第二次 `stride=1`（保持尺寸）。

---

## 2. ResNet 主体结构

### 2.1 基本流程

1. **预处理层**

   * 使用一个大卷积核（如 `7×7, stride=2`）+ BN + ReLU，快速压缩空间尺寸并提取低层特征。
   * 通常后接一次 MaxPool。

2. **残差层堆叠**

   * 四个阶段（Layer1\~Layer4），每个阶段由若干个残差块堆叠。
   * 不同层之间用 stride 控制下采样，逐步减小空间分辨率、增加通道数。

3. **分类头**

   * 使用全局平均池化 (`AdaptiveAvgPool2d((1,1))`)，将不同尺寸的特征图归一化到固定大小。
   * `Flatten` 展平，接入全连接层 (`Linear`) 输出分类结果。

---

## 3. 总结体会

* **残差的核心**：解决深层网络训练时梯度消失的问题，通过「恒等映射」保证信息和梯度能直接传递。
* **实现关键点**：在 **通道数、空间尺寸不一致时如何处理残差分支**。
* **工程技巧**：使用 `nn.Sequential` 管理残差块层数，使用 `AdaptiveAvgPool` 避免手工计算展平维度。

---

👉 整个实现过程，其实就是：**搭建一个基础 ResidualBlock → 堆叠多个 Block → 在合适位置做下采样和通道数扩展 → 加分类头**。

---
 