---

# Transformer å¤ç°ç¬”è®°

è®ºæ–‡ç¿»è¯‘å‚è€ƒï¼š [https://zhuanlan.zhihu.com/p/703292893](https://zhuanlan.zhihu.com/p/703292893)

è®ºæ–‡åœ°å€ï¼š [https://dl.acm.org/doi/pdf/10.5555/3295222.3295349](https://dl.acm.org/doi/pdf/10.5555/3295222.3295349)

ä»£ç å‚è€ƒï¼š [https://blog.csdn.net/nocml/article/details/110920221](https://blog.csdn.net/nocml/article/details/110920221)

---

## ä½ç½®ç¼–ç å™¨ PositionalEncoding

æœ€ç»ˆå¾—åˆ°çš„ **PE** æ˜¯ä¸€ä¸ª `(max_length, d_model)` çš„å¼ é‡ã€‚

### å…¬å¼

![å…¬å¼](img.png)

### å®ç°æ­¥éª¤

**1. åˆ†æ¯éƒ¨åˆ†è®¡ç®—ï¼š**

å‡ ç§å®ç°æ–¹å¼ï¼š

* **æ–¹æ³•1ï¼ˆå¸¸è§å†™æ³•ï¼‰ï¼š**

```python
div_term = torch.exp(torch.arange(0, d_model, 2).float()
                     * -(math.log(10000.0) / d_model))
```

* **æ–¹æ³•2ï¼ˆé€ä¸ª powï¼‰ï¼š**

```python
div_term = torch.pow(10000, -torch.arange(0, d_model, 2).float() / d_model)
```

* **æ–¹æ³•3ï¼ˆæ¨èï¼Œå‘é‡åŒ–ï¼‰ï¼š**

```python
div_term = 10000 ** (-torch.arange(0, d_model, 2).float() / d_model)
```

---

**2. æ­£ä½™å¼¦è®¡ç®—ï¼š**

```python
pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®
pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®
```

---

**3. register\_buffer**

```python
self.register_buffer("pe", pe)
```

* è®© `pe` éš `.to(device)` è‡ªåŠ¨è¿ç§»
* ä¸ä¼šä½œä¸ºå¯è®­ç»ƒå‚æ•°æ›´æ–°

---

## ğŸ”¹ Scaled Dot-Product Attention

**ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot-Product Attentionï¼‰** æ˜¯æœ€å¸¸ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ **Qï¼ˆQueryï¼‰ã€Kï¼ˆKeyï¼‰ã€Vï¼ˆValueï¼‰** çš„è®¡ç®—æ¥è·å¾—æ³¨æ„åŠ›åˆ†å¸ƒã€‚

è®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š

```
Q Ã— Káµ€ â†’ ç¼©æ”¾ (scale) â†’ æ©ç  (mask) â†’ Softmax â†’ ä¸ V ç›¸ä¹˜
```

![img\_1.png](img_1.png)

---

### ğŸ“Œ å…³é”®æ­¥éª¤è¯´æ˜

1. **ç‚¹ç§¯ (Q Ã— Káµ€)**

   * `K` è½¬ç½®åå†ä¸ `Q` ç‚¹ç§¯ï¼Œç»“æœè¡¨ç¤ºç›¸ä¼¼åº¦ã€‚

2. **ç¼©æ”¾ (Scaling)**

   * é™¤ä»¥ `âˆšd_k` é¿å…æ•°å€¼è¿‡å¤§ï¼Œå‡ç¼“æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

3. **æ©ç  (Masking)**

   * å°† padding æˆ–æœªæ¥æ—¶åˆ»çš„åˆ†æ•°ç½®ä¸º `-1e9`ï¼Œä½¿å…¶åœ¨ Softmax åè¶‹è¿‘ 0ã€‚

4. **Softmax**

   * å°†åˆ†æ•°å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚

5. **åŠ æƒæ±‚å’Œ**

   * ä½¿ç”¨ Softmax æƒé‡å¯¹ `V` åŠ æƒï¼Œå¾—åˆ°æ³¨æ„åŠ›è¾“å‡ºã€‚

---

### âœ¨ æ€»ç»“

* **Scaled Dot-Product Attention** æ˜¯ Transformer çš„æ ¸å¿ƒæ“ä½œã€‚
* å®ƒé€šè¿‡ **ç¼©æ”¾ + æ©ç  + Softmax** å¾—åˆ°æƒé‡ï¼Œå†ä¸ `V` ç›¸ä¹˜ã€‚
* Multi-Head Attention æ­£æ˜¯åŸºäºè¯¥æœºåˆ¶çš„æ‰©å±•ã€‚

---

## ğŸ”¹ Multi-Head Attention

å¤šå¤´æ³¨æ„åŠ›é€šè¿‡ **å¹¶è¡Œå¤šç»„ Q/K/V æŠ•å½±**ï¼Œè®©æ¨¡å‹åœ¨ä¸åŒå­ç©ºé—´ä¸ŠåŒæ—¶å­¦ä¹ æ³¨æ„åŠ›è¡¨ç¤ºã€‚

### ğŸ“Œ å­¦ä¹ è¿‡ç¨‹ä¸­çš„å¸¸è§é”™è¯¯ä¸ç¼ºæ¼

1. **Linear å±‚å‚æ•°è®¾ç½®é”™è¯¯**

   * é”™è¯¯ï¼šä»¥ä¸º `nn.Linear` ä¸éœ€è¦æŒ‡å®šè¾“å…¥/è¾“å‡ºç»´åº¦ã€‚
   * ä¿®æ­£ï¼šè¾“å…¥æ˜¯ `[batch, seq_len, d_model]`ï¼Œå› æ­¤ `nn.Linear(d_model, d_model)`ã€‚

2. **å¿˜è®°æ‹†åˆ†å¤šå¤´**

   * é”™è¯¯ï¼šç›´æ¥å¯¹ `Q/K/V` åšæ³¨æ„åŠ›è®¡ç®—ã€‚
   * ä¿®æ­£ï¼šå¿…é¡» `view(nbatches, seq_len, h, d_k)` å† `transpose(1,2)` â†’ `[batch, h, seq_len, d_k]`ã€‚

3. **mask ç»´åº¦å¤„ç†é”™è¯¯**

   * é”™è¯¯ï¼šmask ä¸ `scores` ä¸å¯¹é½ï¼Œå¯¼è‡´ `RuntimeError`ã€‚
   * ä¿®æ­£ï¼šmask éœ€è¦ `unsqueeze(1)` å˜æˆ `[batch, 1, 1, seq_len]`ã€‚

4. **å‚æ•°å…±äº«é—®é¢˜**

   * é”™è¯¯ï¼šä½¿ç”¨ `[nn.Linear()] * 4`ï¼Œå¯¼è‡´ Q/K/V å…±äº«å‚æ•°ã€‚
   * ä¿®æ­£ï¼šå¿…é¡» `nn.ModuleList([nn.Linear(...) for _ in range(4)])`ï¼Œæˆ–ç”¨ `copy.deepcopy`ã€‚

5. **attention ä¸­ç¼ºå°‘ç¼©æ”¾**

   * é”™è¯¯ï¼šæœ‰æ—¶å¿˜è®°é™¤ä»¥ `âˆšd_k`ã€‚
   * ä¿®æ­£ï¼š`scores = torch.matmul(Q, Káµ€) / math.sqrt(d_k)`ã€‚

6. **æ‹¼æ¥å¤šå¤´æ—¶å¿˜è®° contiguous**

   * é”™è¯¯ï¼šç›´æ¥ reshape å¯èƒ½æŠ¥é”™æˆ–é¡ºåºé”™è¯¯ã€‚
   * ä¿®æ­£ï¼š`x.transpose(1,2).contiguous().view(batch, seq_len, d_model)`ã€‚

---

### âœ… ä»£ç é€»è¾‘æ¢³ç†

1. `Linear` æŠ•å½±å¾—åˆ° Q/K/Vã€‚
2. reshape & transpose â†’ `[batch, h, seq_len, d_k]`ã€‚
3. è®¡ç®— **Scaled Dot-Product Attention**ã€‚
4. æ‹¼æ¥å¤šå¤´è¾“å‡º â†’ `[batch, seq_len, d_model]`ã€‚
5. é€šè¿‡æœ€åä¸€ä¸ª `Linear` æŠ•å½±ï¼Œä¿æŒè¾“å‡ºç»´åº¦ä¸€è‡´ã€‚

---

### âœ¨ æ€»ç»“

* **é”™è¯¯ä¸»è¦é›†ä¸­åœ¨ç»´åº¦å¤„ç†ã€mask å¹¿æ’­ã€Linear å‚æ•°å…±äº«**ã€‚
* å¤šå¤´æ³¨æ„åŠ›çš„æ ¸å¿ƒæ˜¯ï¼š

  ```
  Linear â†’ æ‹†åˆ†å¤šå¤´ â†’ Attention â†’ æ‹¼æ¥å¤šå¤´ â†’ Linear
  ```
* ä¿è¯æ¯ä¸ªæ­¥éª¤ç»´åº¦æ­£ç¡®ï¼Œæ‰èƒ½å®ç°ç¨³å®šçš„ Transformer å¤ç°ã€‚

---

## ğŸ”¹ Layer Normalization (LN)

**Layer Normalization** ä¸ **Batch Normalization** ç±»ä¼¼ï¼Œéƒ½æ˜¯å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿æ¨¡å‹è®­ç»ƒæ›´ç¨³å®šã€‚ä¸åŒç‚¹åœ¨äºï¼š

* **BatchNorm**ï¼šå¯¹ batch ç»´åº¦ç»Ÿè®¡å‡å€¼å’Œæ–¹å·®ï¼Œä¾èµ–äº batch sizeã€‚
* **LayerNorm**ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„ **ç‰¹å¾ç»´åº¦**ï¼ˆå¦‚ `d_model`ï¼‰ç»Ÿè®¡å‡å€¼å’Œæ–¹å·®ï¼Œä¸ä¾èµ– batch sizeã€‚

å› æ­¤ï¼ŒTransformer ä¸­æ›´é€‚åˆä½¿ç”¨ **LayerNorm**ï¼Œå› ä¸º NLP ä»»åŠ¡å¸¸å¸¸ batch size è¾ƒå°ï¼Œè€Œ LN ä¸ batch size æ— å…³ã€‚

---

### ğŸ“Œ å…¬å¼

å¯¹è¾“å…¥å¼ é‡ `x âˆˆ R^(batch, seq_len, d_model)`ï¼Œåœ¨æœ€åä¸€ç»´ `d_model` ä¸Šåšå½’ä¸€åŒ–ï¼š

![img_2.png](img_2.png)
---

### ğŸ“Œ ä»£ç å®ç°æ³¨æ„ç‚¹

1. **å‡å€¼å’Œæ–¹å·®è®¡ç®—**

   ```python
   mean = x.mean(dim=-1, keepdim=True)
   var = x.var(dim=-1, keepdim=True, unbiased=False)
   std = torch.sqrt(var + self.eps)
   ```

   * å¿…é¡» `keepdim=True`ï¼Œä¿è¯å½¢çŠ¶å¯å¹¿æ’­ã€‚
   * `unbiased=False`ï¼Œé™¤ä»¥ `N`ï¼Œé¿å…å° batch æ—¶æ•°å€¼ä¸ç¨³å®šã€‚

2. **gamma å’Œ beta çš„å½¢çŠ¶**

   ```python
   self.gamma = nn.Parameter(torch.ones(features))
   self.beta = nn.Parameter(torch.zeros(features))
   ```

   * åªéœ€ä¸€ç»´ `[features]`ï¼ŒPyTorch å¹¿æ’­æœºåˆ¶ä¼šè‡ªåŠ¨æ‰©å±•åˆ° `[batch, seq_len, d_model]`ã€‚

3. **forward é€»è¾‘**

   ```python
   return self.gamma * (x - mean) / std + self.beta
   ```

---

### âœ… ä»£ç ç¤ºä¾‹

```python
class LayerNormalization(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNormalization, self).__init__()
        self.gamma = nn.Parameter(torch.ones(features))
        self.beta = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        std = torch.sqrt(var + self.eps)
        return self.gamma * (x - mean) / std + self.beta
```

---

### âœ¨ æ€»ç»“

* **LN ä¸ BN çš„åŒºåˆ«**ï¼šLN ä¸ä¾èµ– batch sizeï¼Œæ›´é€‚åˆ Transformerã€‚
* **å®ç°è¦ç‚¹**ï¼š

  * åœ¨æœ€åä¸€ç»´ä¸Šæ±‚å‡å€¼å’Œæ–¹å·®ã€‚
  * ä½¿ç”¨ `unbiased=False` é¿å…å°æ ·æœ¬ä¸ç¨³å®šã€‚
  * å‚æ•° `gamma`ã€`beta` é€šè¿‡å¹¿æ’­è‡ªåŠ¨æ‰©å±•ã€‚
* LN çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯ **å¯¹æ¯ä¸ªæ ·æœ¬çš„ embedding å‘é‡åšå½’ä¸€åŒ–ï¼Œå†é€šè¿‡å¯å­¦ä¹ å‚æ•°è°ƒæ•´åˆ†å¸ƒ**ã€‚

---
