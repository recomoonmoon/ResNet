---

# Transformer å¤ç°ç¬”è®°

è®ºæ–‡ç¿»è¯‘å‚è€ƒï¼š [https://zhuanlan.zhihu.com/p/703292893](https://zhuanlan.zhihu.com/p/703292893)

è®ºæ–‡åœ°å€ï¼š [https://dl.acm.org/doi/pdf/10.5555/3295222.3295349](https://dl.acm.org/doi/pdf/10.5555/3295222.3295349)

ä»£ç å‚è€ƒï¼š [https://blog.csdn.net/nocml/article/details/110920221](https://blog.csdn.net/nocml/article/details/110920221)

---

## ä½ç½®ç¼–ç å™¨ PositionalEncoding

æœ€ç»ˆå¾—åˆ°çš„ **PE** æ˜¯ä¸€ä¸ª `(max_length, d_model)` çš„å¼ é‡ã€‚

### å…¬å¼

![å…¬å¼](img.png)

### å®ç°æ­¥éª¤

**1. åˆ†æ¯éƒ¨åˆ†è®¡ç®—ï¼š**

å‡ ç§å®ç°æ–¹å¼ï¼š

* **æ–¹æ³•1ï¼ˆå¸¸è§å†™æ³•ï¼‰ï¼š**

```python
div_term = torch.exp(torch.arange(0, d_model, 2).float()
                     * -(math.log(10000.0) / d_model))
```

* **æ–¹æ³•2ï¼ˆé€ä¸ª powï¼‰ï¼š**

```python
div_term = torch.pow(10000, -torch.arange(0, d_model, 2).float() / d_model)
```

* **æ–¹æ³•3ï¼ˆæ¨èï¼Œå‘é‡åŒ–ï¼‰ï¼š**

```python
div_term = 10000 ** (-torch.arange(0, d_model, 2).float() / d_model)
```

---

**2. æ­£ä½™å¼¦è®¡ç®—ï¼š**

```python
pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®
pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®
```

---

**3. register\_buffer**

```python
self.register_buffer("pe", pe)
```

* è®© `pe` éš `.to(device)` è‡ªåŠ¨è¿ç§»
* ä¸ä¼šä½œä¸ºå¯è®­ç»ƒå‚æ•°æ›´æ–°

---

## ğŸ”¹ Scaled Dot-Product Attention

**ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot-Product Attentionï¼‰** æ˜¯æœ€å¸¸ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ **Qï¼ˆQueryï¼‰ã€Kï¼ˆKeyï¼‰ã€Vï¼ˆValueï¼‰** çš„è®¡ç®—æ¥è·å¾—æ³¨æ„åŠ›åˆ†å¸ƒã€‚

è®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š

```
Q Ã— Káµ€ â†’ ç¼©æ”¾ (scale) â†’ æ©ç  (mask) â†’ Softmax â†’ ä¸ V ç›¸ä¹˜
```

![img\_1.png](img_1.png)

---

### ğŸ“Œ å…³é”®æ­¥éª¤è¯´æ˜

1. **ç‚¹ç§¯ (Q Ã— Káµ€)**

   * `K` è½¬ç½®åå†ä¸ `Q` ç‚¹ç§¯ï¼Œç»“æœè¡¨ç¤ºç›¸ä¼¼åº¦ã€‚

2. **ç¼©æ”¾ (Scaling)**

   * é™¤ä»¥ `âˆšd_k` é¿å…æ•°å€¼è¿‡å¤§ï¼Œå‡ç¼“æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

3. **æ©ç  (Masking)**

   * å°† padding æˆ–æœªæ¥æ—¶åˆ»çš„åˆ†æ•°ç½®ä¸º `-1e9`ï¼Œä½¿å…¶åœ¨ Softmax åè¶‹è¿‘ 0ã€‚

4. **Softmax**

   * å°†åˆ†æ•°å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚

5. **åŠ æƒæ±‚å’Œ**

   * ä½¿ç”¨ Softmax æƒé‡å¯¹ `V` åŠ æƒï¼Œå¾—åˆ°æ³¨æ„åŠ›è¾“å‡ºã€‚

---

### âœ¨ æ€»ç»“

* **Scaled Dot-Product Attention** æ˜¯ Transformer çš„æ ¸å¿ƒæ“ä½œã€‚
* å®ƒé€šè¿‡ **ç¼©æ”¾ + æ©ç  + Softmax** å¾—åˆ°æƒé‡ï¼Œå†ä¸ `V` ç›¸ä¹˜ã€‚
* Multi-Head Attention æ­£æ˜¯åŸºäºè¯¥æœºåˆ¶çš„æ‰©å±•ã€‚

---

## ğŸ”¹ Multi-Head Attention

å¤šå¤´æ³¨æ„åŠ›é€šè¿‡ **å¹¶è¡Œå¤šç»„ Q/K/V æŠ•å½±**ï¼Œè®©æ¨¡å‹åœ¨ä¸åŒå­ç©ºé—´ä¸ŠåŒæ—¶å­¦ä¹ æ³¨æ„åŠ›è¡¨ç¤ºã€‚

### ğŸ“Œ å­¦ä¹ è¿‡ç¨‹ä¸­çš„å¸¸è§é”™è¯¯ä¸ç¼ºæ¼

1. **Linear å±‚å‚æ•°è®¾ç½®é”™è¯¯**

   * é”™è¯¯ï¼šä»¥ä¸º `nn.Linear` ä¸éœ€è¦æŒ‡å®šè¾“å…¥/è¾“å‡ºç»´åº¦ã€‚
   * ä¿®æ­£ï¼šè¾“å…¥æ˜¯ `[batch, seq_len, d_model]`ï¼Œå› æ­¤ `nn.Linear(d_model, d_model)`ã€‚

2. **å¿˜è®°æ‹†åˆ†å¤šå¤´**

   * é”™è¯¯ï¼šç›´æ¥å¯¹ `Q/K/V` åšæ³¨æ„åŠ›è®¡ç®—ã€‚
   * ä¿®æ­£ï¼šå¿…é¡» `view(nbatches, seq_len, h, d_k)` å† `transpose(1,2)` â†’ `[batch, h, seq_len, d_k]`ã€‚

3. **mask ç»´åº¦å¤„ç†é”™è¯¯**

   * é”™è¯¯ï¼šmask ä¸ `scores` ä¸å¯¹é½ï¼Œå¯¼è‡´ `RuntimeError`ã€‚
   * ä¿®æ­£ï¼šmask éœ€è¦ `unsqueeze(1)` å˜æˆ `[batch, 1, 1, seq_len]`ã€‚

4. **å‚æ•°å…±äº«é—®é¢˜**

   * é”™è¯¯ï¼šä½¿ç”¨ `[nn.Linear()] * 4`ï¼Œå¯¼è‡´ Q/K/V å…±äº«å‚æ•°ã€‚
   * ä¿®æ­£ï¼šå¿…é¡» `nn.ModuleList([nn.Linear(...) for _ in range(4)])`ï¼Œæˆ–ç”¨ `copy.deepcopy`ã€‚

5. **attention ä¸­ç¼ºå°‘ç¼©æ”¾**

   * é”™è¯¯ï¼šæœ‰æ—¶å¿˜è®°é™¤ä»¥ `âˆšd_k`ã€‚
   * ä¿®æ­£ï¼š`scores = torch.matmul(Q, Káµ€) / math.sqrt(d_k)`ã€‚

6. **æ‹¼æ¥å¤šå¤´æ—¶å¿˜è®° contiguous**

   * é”™è¯¯ï¼šç›´æ¥ reshape å¯èƒ½æŠ¥é”™æˆ–é¡ºåºé”™è¯¯ã€‚
   * ä¿®æ­£ï¼š`x.transpose(1,2).contiguous().view(batch, seq_len, d_model)`ã€‚

---

### âœ… ä»£ç é€»è¾‘æ¢³ç†

1. `Linear` æŠ•å½±å¾—åˆ° Q/K/Vã€‚
2. reshape & transpose â†’ `[batch, h, seq_len, d_k]`ã€‚
3. è®¡ç®— **Scaled Dot-Product Attention**ã€‚
4. æ‹¼æ¥å¤šå¤´è¾“å‡º â†’ `[batch, seq_len, d_model]`ã€‚
5. é€šè¿‡æœ€åä¸€ä¸ª `Linear` æŠ•å½±ï¼Œä¿æŒè¾“å‡ºç»´åº¦ä¸€è‡´ã€‚

---

### âœ¨ æ€»ç»“

* **é”™è¯¯ä¸»è¦é›†ä¸­åœ¨ç»´åº¦å¤„ç†ã€mask å¹¿æ’­ã€Linear å‚æ•°å…±äº«**ã€‚
* å¤šå¤´æ³¨æ„åŠ›çš„æ ¸å¿ƒæ˜¯ï¼š

  ```
  Linear â†’ æ‹†åˆ†å¤šå¤´ â†’ Attention â†’ æ‹¼æ¥å¤šå¤´ â†’ Linear
  ```
* ä¿è¯æ¯ä¸ªæ­¥éª¤ç»´åº¦æ­£ç¡®ï¼Œæ‰èƒ½å®ç°ç¨³å®šçš„ Transformer å¤ç°ã€‚

---
 